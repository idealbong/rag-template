# ==============================
# Backend Configuration (.env)
# ==============================

# ------------------------------
# Run Server
# ------------------------------
SERVER_HOST=0.0.0.0
CLIENT_HOST=127.0.0.1
PORT=8000
RELOAD=true
EXPOSE_RETRIEVE_ENDPOINT=true
FRONTEND_HOST=http://localhost:5173

# ------------------------------
# Retrieval
# ------------------------------
VECTOR_DB=faiss
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
FAISS_INDEX_DIR=data/faiss_index
USE_CUDA=false
FAISS_DISTANCE_STRATEGY=cosine  # or euclidean

TOP_K=3
RERANK_ENABLED=true
CROSS_ENCODER_MODEL='dragonkue/bge-reranker-v2-m3-ko'
RERANK_CANDIDATES=10

# ------------------------------
# LLM
# ------------------------------
LLM_PROVIDER=gguf  # huggingface or gguf or openai
LLM_MAX_TOKENS=4096
LLM_CONTEXT_LENGTH=32768
LLM_TEMPERATURE=0.7
LLM_TOP_P=0.8
LLM_SYSTEM_PROMPT="당신은 질문 내용에 답변할 수 있는 전문가입니다. 주어진 문서를 바탕으로 정확하고 도움이 되는 답변을 한국어로 제공해주세요. 문서에 없는 내용은 추측하지 말고, 문서 내용만을 바탕으로 답변해주세요."
LLM_USER_PROMPT_TEMPLATE="참고 문서:\n{context}\n\n질문: {question}:"

# ------------------------------
# GGUF Model
# ------------------------------
GGUF_MODEL_PATH=data/A.X-4.0-Light-Q4_K_M.gguf
#GGUF_MODEL_PATH=data/Qwen2.5-14B-Instruct.Q4_K_S.gguf
#GGUF_MODEL_PATH=data/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf
GGUF_THREADS=8
GGUF_GPU_LAYERS=45
GGUF_BATCH_SIZE=512


# ------------------------------
# Huggingface Model
# ------------------------------
HUGGINGFACE_MODEL_ID=skt/A.X-4.0-Light

# ------------------------------
# OpenAI Model
# ------------------------------
OPENAI_MODEL_NAME=gpt-5-nano
OPENAI_API_KEY=sk-proj-***
TOKENIZERS_PARALLELISM=false