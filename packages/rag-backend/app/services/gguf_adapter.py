from dotenv import load_dotenv
import os

from langchain_community.chat_models.llamacpp import ChatLlamaCpp

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class GGUFAdapter():
    def __init__(self):
        # Initialize GGUF specific settings
        self.model = None
        self.model_path = os.getenv("GGUF_MODEL_PATH", "data/Qwen2.5-14B-Instruct-Q4_K_S.gguf")
        self.model_name = self.model_path.split("/")[-1].replace(".gguf", "")
        self.context_length = int(os.getenv("LLM_CONTEXT_LENGTH", 2048))
        self.threads = int(os.getenv("GGUF_THREADS", 4))
        self.gpu_layers = int(os.getenv("GGUF_GPU_LAYERS", 0))
        self.batch_size = int(os.getenv("GGUF_BATCH_SIZE", 512))
        
    def load(self) -> ChatLlamaCpp:
        # Load the model from gguf
            
        try:
            print(f"ğŸ¤– Loading GGUF model: {self.model_path}")
            print("â³ This may take a few minutes...")
            
            # GGUF ëª¨ë¸ ë¡œë“œ (í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
            self.model = ChatLlamaCpp(
                name=self.model_name,
                model_path=self.model_path,
                n_ctx=self.context_length,
                n_threads=self.threads,
                n_gpu_layers=self.gpu_layers,
                n_batch=self.batch_size
            )
            
            self.model_loaded = True
            print(f"âœ… model {self.model_name} loaded successfully")
            
        except Exception as e:
            print(f"âŒ Error loading model: {str(e)}")
            print("ğŸ’¡ Fallback to template-based responses")
            self.model = None
            self.model_loaded = False
            
        return self.model
    
    def get_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "model_provider": "gguf",
            "model_path": self.model_path,
            "status": "loaded" if self.model_loaded else "template_mode",
            "context_length": self.context_length,
            "threads": self.threads,
            "gpu_layers": self.gpu_layers,
            "batch_size": self.batch_size
        }
        
    def get_invoke_kwargs(self) -> dict:
        """LLM í˜¸ì¶œ ì‹œ ì‚¬ìš©í•  ì¶”ê°€ ì¸ìë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "temperature": float(os.getenv("LLM_TEMPERATURE", 0.3)),
            "top_p": float(os.getenv("LLM_TOP_P", 0.8)),
        }